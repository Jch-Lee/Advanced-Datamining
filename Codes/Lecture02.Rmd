---
title: "Lecture02"
author: "JongCheolLee"
date: "2024-09-23"
output:
  pdf_document:
    latex_engine: xelatex
mainfont: NanumGothic
---

```{r}
set.seed(123)
n <- 100
pp <- c(10, 50, 80, 95, 97, 98, 99) # 변수 개수를 바꿔가며 실험
B <- matrix(0, 100, length(pp))

for (i in 1:100) { # 100번 반복하며 추정
  for (j in 1:length(pp)) {
    beta <- rep(0, pp[j])
    beta[1] <- 1 # Beta1 외의 가중치는 모두 0. Bias = E(Beta1_hat) - 1
    x <- matrix(rnorm(n*pp[j]), n, pp[j])
    y <- x %*% beta + rnorm(n) # True Linear Model
    g <- lm(y~x) # Estimation
    B[i,j] <- g$coef[2] # 추정한 계수들 중, Beta1_hat만 저장
  }
}

boxplot(B, col="orange", boxwex=0.6, ylab="Coefficient estimates",
names=pp, xlab="The number of predictors", ylim=c(-5,5))
abline(h=1, col=2, lty=2, lwd=2)
```


```{r}
apply(B, 2, mean)
```

* LSE의 unbiased 성질 덕분에 모두 평균은 1에 근접함

```{r}
apply(B, 2, var)
```

* 하지만 변수가 많아질수록 B1의 추정치의 분산이 매우 커진다.

### Best Subset Selection

```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
  # 종속변수가 비어있는 애들
```

```{r}
Hitters <- na.omit(Hitters)
```

```{r}
dim(Hitters)
```

```{r}
sum(is.na(Hitters))
```

```{r}
library(leaps)
fit <- regsubsets(Salary ~ ., Hitters)
  # best regression subset을 고르는 함수
summary(fit)
```

```{r}
sg <- summary(fit)
names(sg)
```

```{r}
dim(sg$which)
```

```{r}
sg$which

```

```{r}
plot(fit) # default: bic
```

```{r}
plot(fit, scale="Cp")

```

```{r}
big <- regsubsets(Salary ~ ., data=Hitters, nvmax=19, nbest=10)
  # 변수를 8개까지만 보지말고, 19개까지 모두 비교
  # 가장 좋은 10개만 저장
sg <- summary(big)
dim(sg$which)
```

* best를 뽑는 방식: rss, rsq

```{r}
sg.size <- as.numeric(rownames(sg$which))
table(sg.size)
```

```{r}
sg.rss <- tapply(sg$rss, sg.size, min)
w1 <- which.min(sg.rss)
sg.rsq <- tapply(sg$rsq, sg.size, max)
w2 <- which.max(sg.rsq)
par(mfrow=c(1,2))
plot(1:19, sg.rss, type="b", xlab="Number of Predictors",
ylab="Residual Sum of Squares", col=2, pch=19)
points(w1, sg.rss[w1], pch="x", col="blue", cex=2)
plot(1:19, sg.rsq, type="b", xlab="Number of Predictors",
ylab=expression(R^2), col=2, pch=19)
points(w2, sg.rsq[w2], pch="x", col="blue", cex=2)

```

* RSS, R^2의 특징은 모델에 포함되는 변수를 늘릴수록 무조건 줄어든다.
* 따라서 서로 다른 모델 간의 비교에는 부적절하다.
* 하지만 같은 모델 내에서의 비교에는 적절한 방법

### Forward, Backward selection

```{r}
g.full <- regsubsets(Salary ~., data=Hitters)
g.forw <- regsubsets(Salary ~., data=Hitters, method="forward")
g.back <- regsubsets(Salary ~., data=Hitters, method="backward")
full <- summary(g.full)$which[,-1]
full[full==TRUE] <- 1
forw <- summary(g.forw)$which[,-1]
forw[forw==TRUE] <- 1
back <- summary(g.back)$which[,-1]
back[back==TRUE] <- 1

```

```{r}
full
```

```{r}
forw
```

```{r}
back
```

```{r}
coef(g.full, 1:5)
```

```{r}
coef(g.forw, 1:5)
```

```{r}
coef(g.back, 1:5)
```

### Cp(AIC), BIC, adj_R^2

```{r}
sg.cp <- tapply(sg$cp, sg.size, min)
w3 <- which.min(sg.cp)
sg.bic <- tapply(sg$bic, sg.size, min)
w4 <- which.min(sg.bic)
sg.adjr2 <- tapply(sg$adjr2, sg.size, max)
w5 <- which.max(sg.adjr2)
par(mfrow=c(1,3))
plot(1:19, sg.cp, type="b", xlab ="Number of Predictors",
ylab=expression(C[p]), col=2, pch=19)
points(w3, sg.cp[w3], pch="x", col="blue", cex=2)
plot(1:19, sg.bic, type="b", xlab ="Number of Predictors",
ylab="Bayesian information criterion", col=2, pch=19)
points(w4, sg.bic[w4], pch="x", col="blue", cex=2)
plot(1:19, sg.adjr2, type="b", xlab ="Number of Predictors",
ylab=expression(paste("Adjusted ", R^2)), col=2, pch=19)
points(w5, sg.adjr2[w5], pch="x", col="blue", cex=2)

```

```{r}
model1 <- coef(big, which.min(sg$rss))
model2 <- coef(big, which.max(sg$rsq))
model3 <- coef(big, which.max(sg$adjr2))
model4 <- coef(big, which.min(sg$cp))
model5 <- coef(big, which.min(sg$bic))
RES <- matrix(0, 20, 5)
rownames(RES) <- names(model1)
colnames(RES) <- c("rss", "rsq", "adjr2", "cp", "bic")
for (i in 1:5) {
  model <- get(paste("model", i, sep=""))
  w <- match(names(model), rownames(RES))
  RES[w, i] <- model
}
RES
```

```{r}
apply(RES, 2, function(t) sum(t!=0)-1)
```

### Validation Set

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace=TRUE)
test <- (!train)
g1 <- regsubsets(Salary ~ ., data=Hitters[train, ], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[test, ])
val.errors <- rep(NA, 19)

for (i in 1:19) {
  coefi <- coef(g1, id=i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- sqrt(mean((Hitters$Salary[test]-pred)^2))
}
val.errors
```

```{r}
w <- which.min(val.errors)
par(mfrow=c(1,2))
plot(1:19, val.errors, type="l", col="red",
xlab="Number of Predictors", ylab="Validation Set Error")
points(1:19, val.errors, pch=19, col="blue")
points(w, val.errors[w], pch="x", col="blue", cex=2)

```

### K-fold validation

```{r}
set.seed(1234)
N <- 8
ERR <- matrix(0, 19, N)
for (k in 1:N) {
  tr <- sample(c(TRUE, FALSE), nrow(Hitters), replace=TRUE)
  tt <- (!tr)
  g <- regsubsets(Salary ~ ., data=Hitters[tr, ], nvmax=19)
  tt.mat <- model.matrix(Salary~., data=Hitters[tt, ])
  for (i in 1:19) {
    coefi <- coef(g, id=i)
    pred <- tt.mat[, names(coefi)] %*% coefi
    ERR[i,k] <- sqrt(mean((Hitters$Salary[tt]-pred)^2))
  }
}
matplot(ERR, type="l", col="red", xlab="Number of Predictors",
lty=1, ylab="Validation Set Error")

```

* Validation Set이 바뀔 때마다 매우 다른 결과
* 해결책은 교차검증!

```{r}
apply(ERR, 2, which.min)
```

```{r}
## Define new "predict" function on regsubset
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
set.seed(1)
K <- 10
n <- nrow(Hitters)
fd <- sample(rep(1:K, length=n))
cv.errors <- matrix(NA , n, 19, dimnames=list(NULL, paste(1:19)))

for (i in 1:K) {
  fit <- regsubsets(Salary~., Hitters[fd!=i, ], nvmax=19)
  for (j in 1:19) {
    pred <- predict(fit, Hitters[fd==i, ], id=j)
    cv.errors[fd==i, j] <- (Hitters$Salary[fd==i]-pred)^2
  }
}

sqrt(apply(cv.errors, 2, mean))

```

```{r}
K.ERR <- sqrt(apply(cv.errors, 2, mean))
ww <- which.min(K.ERR)
par(mfrow=c(1,2))
plot(1:19, K.ERR, type="l", col="red",
xlab="Number of Predictors", ylab="Cross-Validation Error")
points(1:19, K.ERR, pch=19, col="blue")
points(ww, K.ERR[ww], pch="x", col="blue", cex=2)
```

```{r}
## 10-fold CV with 8 different splits
N <- 8
n <- nrow(Hitters)
ERR <- matrix(0, 19, N)
set.seed(1234)
for (k in 1:N) {
  fd <- sample(rep(1:K, length=n))
  CVR <- matrix(NA , n, 19)
  for (i in 1:K) {
    f <- regsubsets(Salary~., data=Hitters[fd!=i, ], nvmax=19)
    for (j in 1:19) {
      pred <- predict(f, Hitters[fd==i, ], id=j)
      CVR[fd==i, j] <- (Hitters$Salary[fd==i]-pred)^2
    }
  }
  ERR[,k] <- sqrt(apply(CVR, 2, mean))
}
matplot(ERR, type="l", col="red", xlab="Number of Predictors",
lty=1, ylab="Cross-Validation Error")
```

```{r}
apply(ERR, 2, which.min)
```

```{r}
set.seed(111)
fd <- sample(rep(1:K, length=n))
CVR.1se <- matrix(NA, n, 19)
for (i in 1:K) {
  fit <- regsubsets(Salary~., Hitters[fd!=i, ], nvmax=19)
  for (j in 1:19) {
    pred <- predict(fit, Hitters[fd==i, ], id=j)
    CVR.1se[fd==i, j] <- Hitters$Salary[fd==i]-pred
  }
}
avg <- sqrt(apply(CVR.1se^2, 2, mean))
se <- apply(CVR.1se, 2, sd)/sqrt(n)
PE <- cbind(avg - se, avg, avg + se)
data.frame(lwr=PE[,1], mean=PE[,2], upp=PE[,3])
```

```{r}
which.min(PE[,2])
```

```{r}
w <- which.min(PE[,2])
which(PE[w, 1] < PE[,2] & PE[w, 3] > PE[,2])
```

```{r}
min(which(PE[w, 1] < PE[,2] & PE[w, 3] > PE[,2]))
```

```{r}
matplot(1:19, PE, type="b", col=c(1,2,1), lty=c(3,1,3), pch=20,
xlab="Number of Predictors", ylab="Cross-Validation Error")
abline(h=PE[w, 1], lty=3, col="gray")
abline(h=PE[w, 3], lty=3, col="gray")
points(which.min(avg), PE[which.min(avg),2],
pch="o",col="blue",cex=2)
up <- which(PE[,2] < PE[which.min(PE[,2]),3])
points(min(up), PE[min(up),2], pch="x", col="blue", cex=2)

```

* CV Error가 가장 작은 10번째 값의 upper, lower 바운드 내에서 가장 가벼운 모델인 8번째 모델을 선택해야한다.

### Shrinkage Methods
* **shrinkage** 뜻: 규제, 제약, 압축
* **Ridge**: 계수들의 *L2-norm penalty*를 제약식에 추가
  * 파라미터 *lambda*가 커질수록 제약이 강해짐.
* **Lasso**(Least absolute shrinkage and selection operator): *L1-norm penalty*

##### 1. Ridge regression
```{r, warning=FALSE}
library(glmnet)
x0 <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary

grid <- 10^seq(10, -2, length=100)
g1 <- glmnet(x0, y, alpha=0, lambda=grid)

par(mfrow=c(1,2))
plot(g1, "lambda", label=TRUE)

fun <- function(t) sqrt(var(t)*(length(t)-1)/length(t))
sdx <- matrix(apply(x0, 2, fun), dim(x0)[2], dim(x0)[1])

x <- x0/t(sdx)
g2 <- glmnet(x, y, alpha=0, lambda=grid)

plot(g2, "lambda", label=TRUE)
```

* glmnet 사용시 범주형 변수는 더미변수로 인코딩 필수
* 그래프 위의 19는 shrinkage method의 자유도(= 0이 아닌 계수의 개수)

```{r}
data.frame(sd_g1=apply(x0, 2, sd), sd_g2=apply(x, 2, sd))
```

```{r}
names(g2)
```

```{r}
data.frame(lambda=g2$lambda, df=g2$df)
```

```{r}
data.frame(log.lambda=round(log(g2$lambda), 4), df=g2$df)
```

```{r}
dim(coef(g2))

```

```{r}
coef(g2)[, c("s0", "s10", "s20", "s30")]
```

```{r}
g2$lambda[50]
```

```{r}
coef(g2)[,50]
```

```{r}
sqrt(sum(coef(g2)[-1, 50]^2))
```

```{r}
g2$lambda[60]
```

```{r}
coef(g2)[,60]
```

```{r}
sqrt(sum(coef(g2)[-1, 60]^2))
```

```{r}
l2norm <- function(t) sqrt(sum(t^2))
l2 <- apply(coef(g2)[-1,], 2, l2norm)
data.frame(log_lambda=round(log(g2$lambda), 4),
l2norm=round(l2, 4))
```

##### 2. Lasso
* Ridge regression은 중요한 변수를 선택하는 subset selection이 어렵다.
* Ridge와는 달리 계수가 정확히 0이 될 수 있다.
  * Lasso에서 작은 모델은 0인 계수가 더 많은 모델
* 필요없는 계수를 0으로 보냄으로써 변수 선택을 한다.
* 이 방법에선 optimal model을 찾는 게 optimal lambda를 찾는 문제임.

```{r}
g3 <- glmnet(x, y, alpha=1)
par(mfrow=c(1,2))
plot(g3, "lambda", label=TRUE)
plot(g3, "norm", label=TRUE)
```

* 왼쪽 그래프를 보면 lambda가 아주 큰 값에서 작아지면서 0이 아닌 계수가 하나씩 등장함을 알 수 있다.

```{r}
dim(coef(g3))
```

```{r}
coef(g3)[, c("s0", "s10", "s40", "s60")]
```

```{r}
data.frame(lambda=g3$lambda, df=g3$df)
```

```{r}
dim(g3$beta)
```

```{r}
df2 <- apply(g3$beta, 2, function(t) sum(t!=0))
```

```{r}
data.frame(df1=g3$df, df2=df2)
```

### Selecting the Tuning Parameter in shrinkage methods

##### One standard error rule

```{r}
set.seed(123)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
grid <- 10^seq(10, -2, length=100)
r1 <- glmnet(x[train, ], y[train], alpha=0, lambda=grid)
ss <- 0:(length(r1$lambda)-1)
Err <- NULL
for (i in 1:length(r1$lambda)) {
  r1.pred <- predict(r1, s=ss[i], newx=x[test, ])
  Err[i] <- mean((r1.pred - y.test)^2)
}
wh <- which.min(Err)
lam.opt <- r1$lambda[wh]
```

```{r}
r.full <- glmnet(x, y, alpha=0, lambda=grid)
r.full$beta[,wh]
```

```{r}
predict(r.full, type="coefficients", s=lam.opt)
```
* 누가 validation set으로 할당되냐에 따라 결과가 크게 차이날 수 있어서 실제론 CV가 훨씬 안정적이고 좋음.

##### Cross-Validation(One standard error rule)

```{r}
set.seed(1)
cv.r <- cv.glmnet(x, y, alpha=0, nfolds=10)
names(cv.r)
```

```{r}
cbind(cv.r$cvlo, cv.r$cvm, cv.r$cvup)
plot(cv.r)
```

```{r}
log(cv.r$lambda.min)
```

```{r}
log(cv.r$lambda.1se)
```

```{r}
which.min(cv.r$lambda)
```

```{r}
which(cv.r$lambda==cv.r$lambda.1se)
```

```{r}
b.min <- predict(cv.r, type="coefficients", s=cv.r$lambda.min)
b.1se <- predict(cv.r, type="coefficients", s=cv.r$lambda.1se)
cbind(b.min, b.1se)

```

```{r}
c(sqrt(sum(b.min[-1]^2)), sqrt(sum(b.1se[-1]^2)))
```

```{r}
set.seed(2)
cv.l <- cv.glmnet(x, y, alpha=1, nfolds=10)
plot(cv.l)
```

```{r}
log(cv.l$lambda.min)
```

```{r}
log(cv.l$lambda.1se)
```

```{r}
which(cv.l$lambda==cv.l$lambda.min)
```

```{r}
which(cv.l$lambda==cv.l$lambda.1se)
```

```{r}
b.min <- predict(cv.l, type="coefficients", s=cv.l$lambda.min)
b.1se <- predict(cv.l, type="coefficients", s=cv.l$lambda.1se)
cbind(b.min, b.1se)
```

```{r}
c(sum(abs(b.min[-1])), sum(abs(b.1se[-1])))
```

### Ridge: The Bias-Variance tradeoff

```{r}
set.seed(1234)
K <- 100
p <- 40
n <- 50
beta <- runif(p, -1, 1)
lam <- 10^seq(3, -3, length.out=50)
x <- matrix(rnorm(n * p), n, p)
bhat <- array(0, c(p, length(lam), K))
```

```{r}
for (i in 1:K) {
  y <- x %*% beta + rnorm(n)
  fit <- glmnet(x, y, alpha=0, lambda=lam)
  bhat[,,i] <- as.matrix(fit$beta)
}

MSE0 <- Bias0 <- Vars0 <- matrix(0, p, length(lam))

for (k in 1:length(lam)) {
  MS <- (bhat[,k,] - matrix(beta, p, K))^2
  MSE0[,k] <- apply(MS, 1, mean)
  Bias0[,k] <- abs(apply(bhat[,k,], 1, mean) - beta)
  Vars0[,k] <- apply(bhat[,k,], 1, var)
}
```

```{r}
MSE <- apply(MSE0, 2, mean)
Bias <- apply(Bias0, 2, mean)
Vars <- apply(Vars0, 2, mean)
```

```{r}
MAT <- cbind(Bias^2, Vars, MSE)
data.frame(lambda=round(lam, 3), MSE=MAT[,3])
```

```{r}
matplot(MAT, type="l", col=c(3,4,2), lty=1, xaxt="n",
xlab=expression(lambda), ylab="Mean Squared Error")
legend("topright", c("Bias", "Variance", "MSE"), col=c(3,4,2),
lty=1)

w <- which.min(MAT[,3])
abline(v=w, col="gray", lty=2)
points(w, MAT[w, 3], pch=4, col="red", cex=2)
cc <- seq(1, 50, 8)
axis(1, at=cc, labels=round(lam[cc],4))

```

### Lasso: The Bias-Variance Trade Off

```{r}
set.seed(1234)
K <- 100
p <- 40
n <- 50
beta <- runif(p, -1, 1)
lam <- 10^seq(0.7, -4, length.out=50)
x <- matrix(rnorm(n * p), n, p)
bhat <- array(0, c(p, length(lam), K))
for (i in 1:K) {
  y <- x %*% beta + rnorm(n)
  fit <- glmnet(x, y, alpha=1, lambda=lam)
  bhat[,,i] <- as.matrix(fit$beta)
}
MSE0 <- Bias0 <- Vars0 <- matrix(0, p, length(lam))
for (k in 1:length(lam)) {
  MS <- (bhat[,k,] - matrix(beta, p, K))^2
  MSE0[,k] <- apply(MS, 1, mean)
  Bias0[,k] <- abs(apply(bhat[,k,], 1, mean) - beta)
  Vars0[,k] <- apply(bhat[,k,], 1, var)
}

```

```{r}
MSE <- apply(MSE0, 2, mean)
Bias <- apply(Bias0, 2, mean)
Vars <- apply(Vars0, 2, mean)
MAT <- cbind(Bias^2, Vars, MSE)
data.frame(lambda=round(lam, 3), MSE=MAT[,3])

```

```{r}
matplot(MAT, type="l", col=c(3,4,2), lty=1, xaxt="n",
xlab=expression(lambda), ylab="Mean Squared Error")
legend("topright", c("Bias", "Variance", "MSE"), col=c(3,4,2),
lty=1)
w <- which.min(MAT[,3])
abline(v=w, col="gray", lty=2)
points(w, MAT[w, 3], pch=4, col="red", cex=2)
cc <- seq(1, 50, 8)
axis(1, at=cc, labels=round(lam[cc],4))

```

```{r}
MSE.fun <- function(n, p, K, beta, lam, xtest, ytest) {
  yhat0 <- yhat1 <- array(0, c(n, length(lam), K))
  for (i in 1:K) {
    x <- matrix(rnorm(n * p), n, p)
    y <- x %*% beta + rnorm(n)
    g0 <- glmnet(x, y, alpha=0, lambda=lam)
    g1 <- glmnet(x, y, alpha=1, lambda=lam)
    yhat0[1:n, 1:length(lam), i] <- predict(g0, x.test)
    yhat1[1:n, 1:length(lam), i] <- predict(g1, x.test)
  }
  MSE0 <- Bias0 <- Vars0 <- array(0, c(n,length(lam)))
  MSE1 <- Bias1 <- Vars1 <- array(0, c(n,length(lam)))
  for (j in 1:length(lam)) {
    PE0 <-(yhat0[,j,] - matrix(ytest, n, K))^2
    PE1 <-(yhat1[,j,] - matrix(ytest, n, K))^2
    MSE0[ ,j] <- apply(PE0, 1, mean)
    MSE1[ ,j] <- apply(PE1, 1, mean)
    BS0 <- abs(yhat0[,j,] - matrix(ytest, n, K))
    BS1 <- abs(yhat1[,j,] - matrix(ytest, n, K))
    Bias0[,j] <- apply(BS0, 1, mean)
    Bias1[,j] <- apply(BS1, 1, mean)
    Vars0[,j] <- apply(yhat0[,j,], 1, var)
    Vars1[,j] <- apply(yhat1[,j,], 1, var)
  }
  MSE.r <- apply(MSE0, 2, mean)
  MSE.l <- apply(MSE1, 2, mean)
  Bia.r <- apply(Bias0, 2, mean)
  Bia.l <- apply(Bias1, 2, mean)
  Var.r <- apply(Vars0, 2, mean)
  Var.l <- apply(Vars1, 2, mean)
  ridge <- apply(cbind(Bia.r^2, Var.r, MSE.r), 2, rev)
  lasso <- apply(cbind(Bia.l^2, Var.l, MSE.l), 2, rev)
  newlam <- rev(lam)
  return(list(ridge=ridge,lasso=lasso, lambda=newlam))
}

```

```{r}
set.seed(111000)
K <- 10
p <- 120
n <- 100
lam <- 10^seq(1, -3, -0.05)
x.test <- matrix(rnorm(n * p), n, p)
```

```{r}
## The case that all predictors have non-zero coefficients
beta1 <- beta2 <- runif(p, -1, 1)
ytest1 <- x.test %*% beta1 + rnorm(n)
g1 <- MSE.fun(n, p, K, beta1, lam, xtest, ytest1)
RES1 <- cbind(g1$lasso, g1$ridge)

## The case that only 5 predictors have non-zero coefficients
beta2[6:p] <- 0
ytest2 <- x.test %*% beta2 + rnorm(n)
g2 <- MSE.fun(n, p, K, beta2, lam, xtest, ytest2)
RES2 <- cbind(g2$lasso, g2$ridge)

```

```{r}
par(mfrow=c(1,2))
matplot(RES1, type="l", col=c(1,3,2), lty=rep(1:2,each=3),
xlab=expression(lambda), xaxt="n",
ylab="Mean Squared Error")
cc <- c(1, seq(21, 81, 20))
axis(1, at=cc, labels=g1$lambda[cc])
matplot(RES2, type="l", col=c(1,3,2), lty=rep(1:2,each=3),
xlab=expression(lambda), xaxt="n",
ylab="Mean Squared Error")
legend("topright",c("Bias_Lasso", "Variance_Lasso", "MSE_Lasso",
"Bias_Ridge", "Variance_Ridge", "MSE_Ridge"),
col=c(1,3,2), lty=rep(1:2, each=3))
axis(1, at=cc, labels=g2$lambda[cc])

```

### Survival Analysis

```{r}
library(ISLR2)
data(BrainCancer)
```

```{r}
BrainCancer[ ,7:8]
```

```{r}
lapply(BrainCancer[ ,-c(5, 8)], table)
```

```{r}
summary(BrainCancer[ , c(5, 8)])
```

```{r}
attach(BrainCancer)
plot(time, type="h", col=ifelse(status>0, "orange", "lightblue"),
xlab="Patients", ylab="Survival time (Months)")
legend("topleft", c("Uncensored", "Censored"), lty=1,
col=c("orange", "lightblue"))
```

```{r}
d <- sort(unique(time))
d
```

```{r}
q <- r <- NULL
for (i in 1:length(d)) {
  q[i] <- sum(time[status > 0] == d[i]) # 
  r[i] <- sum(time >= d[i])
}
1-q/r # 케플런 마이어 추정치를 구하기 위함
```

```{r}
data.frame(d, q, r, S=cumprod(1-q/r))
```

##### 패키지 사용

```{r}
library(survival)
Surv(time, status)
```

* time과 status point를 함께 표현
* +가 붙어있으면 censored

```{r}
fit <- survfit(Surv(time, status) ~ 1)
data.frame(d=fit$time, q=fit$n.event, r=fit$n.risk, S=fit$surv)
```

```{r}
plot(fit, col="blue", lty=1.5, xlab="Months",
ylab="Estimated Probability of Survival")
```

### Log rank test

```{r}
library(survival)
data(BrainCancer, package="ISLR2")
attach(BrainCancer)

lapply(BrainCancer[,c("sex", "diagnosis", "stereo")], table)
```

```{r}
fit2 <- survfit(Surv(time, status) ~ sex)
plot(fit2, col=c(2, 4), lty=1.5, xlab="Months",
ylab="Estimated Probability of Survival")
legend("topright", levels(sex), col=c(2, 4), lty=1.5)

```

* sample의 생존확률 추정 결과 여자가 더 오래 산다.
* 과연 통계적으로 유의미한 차이인가?(일반화할 수 있는가)

```{r}
survdiff(Surv(time, status) ~ sex)
```

```{r}
fit3 <- survfit(Surv(time, status) ~ stereo)
plot(fit3, col=c(2, 4), lty=1.5, xlab="Months",
ylab="Estimated Probability of Survival")
legend("topright", levels(stereo), col=c(2, 4), lty=1.5)
```

```{r}
survdiff(Surv(time, status) ~ stereo)
```

### Regression Models with a Survival Response
* Cox's Proportional Hazards Model

```{r}
library(survival)
data(BrainCancer, package="ISLR2")
attach(BrainCancer)
```

```{r}
fit.sex <- coxph(Surv(time, status) ~ sex)
summary(fit.sex)
```

```{r}
fit.all <- coxph(Surv(time, status) ~ sex + diagnosis + loc
+ ki + gtv + stereo)
fit.all
```

```{r}
summary(fit.all)
```

```{r}
TT <- data.frame(diagnosis=levels(diagnosis), sex=rep("Female",4),
loc=rep("Supratentorial",4), ki=rep(mean(ki),4),
gtv=rep(mean(gtv),4), stereo=rep("SRT",4))
TT
```

```{r}
SS <- survfit(fit.all, newdata=TT)
plot(SS, xlab="Months", ylab="Survival Probability", col=2:5)
legend("bottomleft", levels(diagnosis), col=2:5, lty=1.5)
```

