---
title: "Final_Regression"
output: html_document
date: "2024-12-15"
---

```{r}
library(ISLR)
data(College)
dim(College)

url.ad <- "https://www.statlearning.com/s/Advertising.csv"
Advertising <- read.csv(url.ad, h=T)

```

```{r}
College <- na.omit(College)
College <- College[,-1]

Advertising <- Advertising[,-1]
```

### EDA

```{r}
# 데이터프레임 각 변수의 분포를 하나씩 시각화하는 함수
plot_distributions <- function(data) {
  # 데이터프레임인지 확인
  if (!is.data.frame(data)) {
    stop("Input must be a data frame.")
  }
  
  # 각 변수의 그래프 개별 출력
  for (col_name in colnames(data)) {
    column <- data[[col_name]]
    
    if (is.numeric(column)) {
      # 숫자형 변수는 히스토그램
      hist(column, main = paste("Histogram of", col_name),
           xlab = col_name, col = "skyblue", border = "white")
    } else if (is.factor(column) || is.character(column)) {
      # 범주형 변수는 막대그래프
      barplot(table(column), main = paste("Barplot of", col_name),
              xlab = col_name, col = "orange", border = "white")
    } else {
      # 다른 데이터 유형은 무시
      message(paste("Skipping column", col_name, "- unsupported type"))
    }
  }
}

plot_distributions(data.frame(apply(College, 2, log)))
```

```{r}
plot_distributions(College)
```

### Dataset setting

```{r}
rename_and_reorder_columns <- function(data, target_column) {
  if (!is.data.frame(data)) {
    stop("Input must be a data frame.")
  }
  
  if (!(target_column %in% colnames(data))) {
    stop(paste("Column", target_column, "not found in the data frame."))
  }
  
  old_names <- colnames(data)
  other_columns <- setdiff(old_names, target_column)
  new_names <- paste0("X", seq_along(other_columns))
  renamed_columns <- setNames(data, c(new_names, "y")[match(old_names, c(other_columns, target_column))])
  reordered_data <- renamed_columns[, c(new_names, "y")]
  
  return(reordered_data)
}

exdata <- rename_and_reorder_columns(College, "Grad.Rate")
exdata2 <- rename_and_reorder_columns(Advertising, "sales")
```

```{r}
# 표준화를 수행하는 함수 정의
standardize_data <- function(train, test, feature_name) {
  # train과 test가 데이터프레임인지 확인
  if (!is.data.frame(train) || !is.data.frame(test)) {
    stop("train과 test는 데이터프레임이어야 합니다.")
  }
  
  # feature_name 열이 존재하는지 확인
  if (!(feature_name %in% colnames(train)) || !(feature_name %in% colnames(test))) {
    stop("feature_name 열이 train 또는 test에 존재하지 않습니다.")
  }
  
  # 1. 훈련 데이터에서 평균과 표준편차 계산
  train_mean <- mean(train[[feature_name]], na.rm = TRUE)
  train_sd <- sd(train[[feature_name]], na.rm = TRUE)
  
  # 2. 훈련 데이터 표준화
  train[[paste0(feature_name, "_scaled")]] <- (train[[feature_name]] - train_mean) / train_sd
  
  # 3. 테스트 데이터 표준화 (훈련 데이터의 평균과 표준편차 사용)
  test[[paste0(feature_name, "_scaled")]] <- (test[[feature_name]] - train_mean) / train_sd
  
  # 4. 결과 반환 (표준화된 train과 test)
  return(list(
    train = train,
    test = test
  ))
}
```

### 5-fold validation setting

```{r}
fold_frame <- function(data) {
  fold <- sample(rep(1:5, nrow(data)%/%5+1), size=nrow(data)) # 성능 평가를 위한 폴드
  performance <- rep(0, max(fold))
  for (i in 1:max(fold)) {
    trainset <- data[fold!=i,]
    validset <- data[fold==i,]
    ##### 1. 모델 훈련 및 validation set 예측 #####
    ##### 2. validation 성능 평가 #####
    # performance[i] <- 성능
  }
  return(mean(performance))
}
```

### Performances

**regression**
```{r}
RMSE <- function(y_actual, y_pred) {
  sqrt(mean((y_actual - y_pred)^2))
}

R_squared <- function(y_actual, y_pred) {
  rss <- sum((y_actual - y_pred)^2)
  tss <- sum((y_actual - mean(y_actual))^2)
  rsq <- 1-(rss/tss)
  return(rsq)
}
```

**classification**
```{r}
accuracy <- function(y_actual, y_pred) {
  sum(y_actual==y_pred)/length(y_actual)
}

f1_score <- function(y_actual, y_pred) {
  true_positive <- sum(y_actual == 1 & y_pred == 1)
  false_positive <- sum(y_actual == 0 & y_pred == 1)
  false_negative <- sum(y_actual == 1 & y_pred == 0)
  
  if ((2 * true_positive + false_positive + false_negative) == 0) {
    return(0)
  }
  return(2 * true_positive / (2 * true_positive + false_positive + false_negative))
}
```

### Functions setting

```{r}
library(glmnet)
library(gbm)
library(randomForest)

stack331 <- function(trainset, testset) {
  stack_fold <- sample(rep(1:5, nrow(trainset)%/%5+1), size=nrow(trainset))
  n_models1 <- 3
  n_models2 <- 3
  
  testx <- testset[, -ncol(testset)]
  
  stage1_cv_pred_mat <- matrix(0, nrow=nrow(trainset), ncol=n_models1)
  stage1_test_pred_list <- lapply(1:n_models1, function(x) matrix(0,
                                                   nrow = nrow(testset),
                                                   ncol = max(stack_fold)))
  
  for (i in 1:max(stack_fold)) {
    fit1 <- cv.glmnet(apply(as.matrix(trainset[stack_fold!=i, -ncol(trainset)]),
                            2, scale),
                      trainset$y[stack_fold!=i],
                      alpha = 1,
                      nfolds = 3)
    fit2 <- gbm(y~.,
                data = trainset[stack_fold!=i,],
                distribution = "gaussian",
                n.trees = 250,
                verbose = F)
    fit3 <- randomForest(y~.,
                         data = trainset[stack_fold!=i,])
    
    stage1_cv_pred_mat[stack_fold==i,1] <-
      predict(fit1, newx = apply(as.matrix(trainset[stack_fold==i, -ncol(trainset)]),
                                 2, scale),
              s = "lambda.1se")
    stage1_cv_pred_mat[stack_fold==i,2] <-
      predict(fit2, newdata = trainset[stack_fold==i,])
    stage1_cv_pred_mat[stack_fold==i,3] <-
      predict(fit3, newdata = trainset[stack_fold==i,])
    
    stage1_test_pred_list[[1]][,i] <- 
      predict(fit1,
              newx = apply(as.matrix(testx),
                           2, scale),
              s = "lambda.1se")
    stage1_test_pred_list[[2]][,i] <- 
      predict(fit2, newdata = testset)
    stage1_test_pred_list[[3]][,i] <- 
      predict(fit3, newdata = testset)
  }
  
  stage1_test_pred_mat <-
    lapply(stage1_test_pred_list, function(t) apply(t, 1, mean))
  stage1_test_pred_mat <-
    sapply(stage1_test_pred_mat, identity)
  
  stage2_trainset <- data.frame(stage1_cv_pred_mat, y=trainset$y)
  stage2_testx <- data.frame(stage1_test_pred_mat)
  
  stage2_cv_pred_mat <- matrix(0, nrow=nrow(trainset), ncol=n_models2)
  stage2_test_pred_list <- lapply(1:n_models2,
                                  function(x) matrix(0, nrow = nrow(testset),
                                                     ncol = max(stack_fold)))
  
  for (i in 1:max(stack_fold)) {
    fit1 <- cv.glmnet(apply(as.matrix(stage2_trainset[stack_fold!=i, -ncol(stage2_trainset)]),
                            2, scale),
                      stage2_trainset$y[stack_fold!=i],
                      alpha = 1,
                      nfolds = 3)
    fit2 <- gbm(y~.,
                data = stage2_trainset[stack_fold!=i,],
                distribution = "gaussian",
                n.trees = 250,
                verbose = F)
    fit3 <- randomForest(y~.,
                         data = stage2_trainset[stack_fold!=i,])
    
    stage2_cv_pred_mat[stack_fold==i,1] <-
      predict(fit1, newx = apply(as.matrix(stage2_trainset[stack_fold==i, -ncol(stage2_trainset)]),
                                 2, scale),
              s = "lambda.1se")
    stage2_cv_pred_mat[stack_fold==i,2] <-
      predict(fit2, newdata = stage2_trainset[stack_fold==i,])
    stage2_cv_pred_mat[stack_fold==i,3] <-
      predict(fit3, newdata = stage2_trainset[stack_fold==i,])
    
    stage2_test_pred_list[[1]][,i] <- 
      predict(fit1, newx = apply(as.matrix(stage2_testx),
                                 2, scale),
              s = "lambda.1se")
    stage2_test_pred_list[[2]][,i] <- 
      predict(fit2, newdata = stage2_testx)
    stage2_test_pred_list[[3]][,i] <- 
      predict(fit3, newdata = stage2_testx)
  }
  
  stage2_test_pred_mat <- 
    lapply(stage2_test_pred_list, function(t) apply(t, 1, mean))
  stage2_test_pred_mat <-
    sapply(stage2_test_pred_mat, identity)
  
  last_trainset <- data.frame(stage2_cv_pred_mat,
                              y = trainset$y)
  last_testset <- data.frame(stage2_test_pred_mat)
  last_pred_mat <- matrix(0, nrow = nrow(testset),
                          ncol = max(stack_fold))
  
  for (i in 1:max(stack_fold)) {
    fit_last <- gbm(y~.,
                    data = last_trainset[stack_fold!=i,],
                    n.trees = 50,
                verbose = F)
    last_pred_mat[,i] <- predict(fit_last,
                                 newdata = last_testset)
  }
  
  return(apply(last_pred_mat, 1, mean))
}

exres <- stack331(exdata[1:700,], exdata[701:nrow(exdata),])
RMSE(exdata[701:nrow(exdata),"y"], exres)
```

```{r}
stack41 <- function(trainset, testset) {
  stack_fold <- sample(rep(1:5, nrow(trainset)%/%5+1), size=nrow(trainset))
  n_models1 <- 4
  
  testx <- testset[, -ncol(testset)]
  
  stage1_cv_pred_mat <- matrix(0, nrow=nrow(trainset), ncol=n_models1)
  stage1_test_pred_list <- lapply(1:n_models1, function(x) matrix(0,
                                                   nrow = nrow(testset),
                                                   ncol = max(stack_fold)))
  
  for (i in 1:max(stack_fold)) {
    fit1 <- cv.glmnet(apply(as.matrix(trainset[stack_fold!=i, -ncol(trainset)]),
                            2, scale),
                      trainset$y[stack_fold!=i],
                      alpha = 1,
                      nfolds = 3)
    fit2 <- gbm(y~.,
                data = trainset[stack_fold!=i,],
                distribution = "gaussian",
                n.trees = 150,
                verbose = F)
    fit3 <- randomForest(y~.,
                         data = trainset[stack_fold!=i,],
                         ntree = 200)
    fit4 <- gbm(y~.,
                data = trainset[stack_fold!=i,],
                distribution = "gaussian",
                n.trees = 50,
                verbose = F)
    
    stage1_cv_pred_mat[stack_fold==i,1] <-
      predict(fit1, newx = apply(as.matrix(trainset[stack_fold==i, -ncol(trainset)]),
                                 2, scale),
              s = "lambda.1se")
    stage1_cv_pred_mat[stack_fold==i,2] <-
      predict(fit2, newdata = trainset[stack_fold==i,])
    stage1_cv_pred_mat[stack_fold==i,3] <-
      predict(fit3, newdata = trainset[stack_fold==i,])
    stage1_cv_pred_mat[stack_fold==i,4] <-
      predict(fit4, newdata = trainset[stack_fold==i,])
    
    stage1_test_pred_list[[1]][,i] <- 
      predict(fit1,
              newx = apply(as.matrix(testx),
                           2, scale),
              s = "lambda.1se")
    stage1_test_pred_list[[2]][,i] <- 
      predict(fit2, newdata = testset)
    stage1_test_pred_list[[3]][,i] <- 
      predict(fit3, newdata = testset)
    stage1_test_pred_list[[4]][,i] <- 
      predict(fit4, newdata = testset)
  }
  
  stage1_test_pred_mat <-
    lapply(stage1_test_pred_list, function(t) apply(t, 1, mean))
  stage1_test_pred_mat <-
    sapply(stage1_test_pred_mat, identity)
  
  last_trainset <- data.frame(stage1_cv_pred_mat,
                              y = trainset$y)
  last_testset <- data.frame(stage1_test_pred_mat)
  last_pred_mat <- matrix(0, nrow = nrow(testset),
                          ncol = max(stack_fold))
  
  for (i in 1:max(stack_fold)) {
    fit_last <- randomForest(y~.,
                    data = last_trainset[stack_fold!=i,],
                    ntree = 50)
    last_pred_mat[,i] <- predict(fit_last,
                                 newdata = last_testset)
  }
  
  return(apply(last_pred_mat, 1, mean))
}
```

### 5-fold testing - stack331 model

```{r}
stack331_5fold <- function(data) {
  fold <- sample(rep(1:5, nrow(data)%/%5+1), size=nrow(data)) # 성능 평가를 위한 폴드
  performance <- rep(0, max(fold))
  
  for (i in 1:max(fold)) {
    trainset <- data[fold!=i,]
    validset <- data[fold==i,]
    y_pred <- rep(0, nrow(validset))
    y_pred <- stack331(trainset = trainset, testset = validset)
    
    performance[i] <- RMSE(y_actual = validset$y,
                           y_pred = y_pred)
  }
  return(mean(performance))
}

stack331_5fold(exdata)
stack331_5fold(exdata2)
```

### 5-fold testing - stack41 model

```{r}
stack41_5fold <- function(data) {
  fold <- sample(rep(1:5, nrow(data)%/%5+1), size=nrow(data)) # 성능 평가를 위한 폴드
  performance <- rep(0, max(fold))
  
  for (i in 1:max(fold)) {
    trainset <- data[fold!=i,]
    validset <- data[fold==i,]
    y_pred <- rep(0, nrow(validset))
    y_pred <- stack41(trainset = trainset, testset = validset)
    
    performance[i] <- RMSE(y_actual = validset$y,
                           y_pred = y_pred)
  }
  return(mean(performance))
}

stack41_5fold(exdata)
stack41_5fold(exdata2)

```

### 5-fold testing - RF

```{r}
library(randomForest)

RF_5fold <- function(data) {
  fold <- sample(rep(1:5, nrow(data)%/%5+1), size=nrow(data)) # 성능 평가를 위한 폴드
  performance <- rep(0, max(fold))
  for (i in 1:max(fold)) {
    trainset <- data[fold!=i,]
    validset <- data[fold==i,]
    
    fit <- randomForest(y~., data = trainset, ntree=100)
    y_pred <- predict(fit, newdata = validset)

    performance[i] <- RMSE(validset$y, y_pred)
  }
  return(mean(performance))
}

RF_5fold(exdata)
RF_5fold(exdata2)
```

### 5-fold testing - GBM

```{r}
library(gbm)

GBM_5fold <- function(data) {
  fold <- sample(rep(1:5, nrow(data)%/%5+1), size=nrow(data)) # 성능 평가를 위한 폴드
  performance <- rep(0, max(fold))
  for (i in 1:max(fold)) {
    trainset <- data[fold!=i,]
    validset <- data[fold==i,]
    
    fit <- gbm(y~., data = trainset, n.trees = 200,
               verbose = F)
    y_pred <- predict(fit, newdata = validset)

    performance[i] <- RMSE(validset$y, y_pred)
  }
  return(mean(performance))
}

GBM_5fold(exdata)
GBM_5fold(exdata2)
```

```{r}

```

